{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72b9ba94e4f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_path = '../data/ch7/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "\n",
    "cifar2_val = [(img, label_map[label]) \n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512, ), # input features, hidden layer size\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, n_out, ) # hidden layer size, output classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [1.0, 2.0, 3.0]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Softmax(dim=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8628641..2.029448].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = img.view(-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4784, 0.5216]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, index = torch.max(out, dim=1)\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5077, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 7.436963\n",
      "Epoch: 1, Loss: 2.969128\n",
      "Epoch: 2, Loss: 7.077729\n",
      "Epoch: 3, Loss: 5.504475\n",
      "Epoch: 4, Loss: 10.688177\n",
      "Epoch: 5, Loss: 1.415906\n",
      "Epoch: 6, Loss: 9.292529\n",
      "Epoch: 7, Loss: 12.116428\n",
      "Epoch: 8, Loss: 6.664639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(out, torch\u001b[38;5;241m.\u001b[39mtensor([label]))\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, \u001b[38;5;28mfloat\u001b[39m(loss)))\n",
      "File \u001b[0;32m~/Deep-Learning-with-PyTorch/venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Deep-Learning-with-PyTorch/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/Deep-Learning-with-PyTorch/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 220\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss))) # Prints the loss for the last image. \n",
    "                                                        # In the next chapter, we will\n",
    "                                                        # improve our output to give an average over\n",
    "                                                        # the entire epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.257707\n",
      "Epoch: 1, Loss: 0.410050\n",
      "Epoch: 2, Loss: 0.353380\n",
      "Epoch: 3, Loss: 0.491312\n",
      "Epoch: 4, Loss: 0.255176\n",
      "Epoch: 5, Loss: 0.425032\n",
      "Epoch: 6, Loss: 0.407701\n",
      "Epoch: 7, Loss: 0.562127\n",
      "Epoch: 8, Loss: 0.359097\n",
      "Epoch: 9, Loss: 0.499740\n",
      "Epoch: 10, Loss: 0.318886\n",
      "Epoch: 11, Loss: 0.962872\n",
      "Epoch: 12, Loss: 0.314391\n",
      "Epoch: 13, Loss: 0.348557\n",
      "Epoch: 14, Loss: 0.364174\n",
      "Epoch: 15, Loss: 0.305631\n",
      "Epoch: 16, Loss: 0.305762\n",
      "Epoch: 17, Loss: 0.414332\n",
      "Epoch: 18, Loss: 0.223709\n",
      "Epoch: 19, Loss: 0.299378\n",
      "Epoch: 20, Loss: 0.278254\n",
      "Epoch: 21, Loss: 0.164422\n",
      "Epoch: 22, Loss: 0.155688\n",
      "Epoch: 23, Loss: 0.285705\n",
      "Epoch: 24, Loss: 0.173727\n",
      "Epoch: 25, Loss: 0.205155\n",
      "Epoch: 26, Loss: 0.329013\n",
      "Epoch: 27, Loss: 0.536953\n",
      "Epoch: 28, Loss: 0.233588\n",
      "Epoch: 29, Loss: 0.320415\n",
      "Epoch: 30, Loss: 0.205343\n",
      "Epoch: 31, Loss: 0.249236\n",
      "Epoch: 32, Loss: 0.358398\n",
      "Epoch: 33, Loss: 0.204784\n",
      "Epoch: 34, Loss: 0.139924\n",
      "Epoch: 35, Loss: 0.125468\n",
      "Epoch: 36, Loss: 0.088745\n",
      "Epoch: 37, Loss: 0.184781\n",
      "Epoch: 38, Loss: 0.103291\n",
      "Epoch: 39, Loss: 0.120783\n",
      "Epoch: 40, Loss: 0.246437\n",
      "Epoch: 41, Loss: 0.106820\n",
      "Epoch: 42, Loss: 0.179025\n",
      "Epoch: 43, Loss: 0.061698\n",
      "Epoch: 44, Loss: 0.057559\n",
      "Epoch: 45, Loss: 0.113642\n",
      "Epoch: 46, Loss: 0.244740\n",
      "Epoch: 47, Loss: 0.105862\n",
      "Epoch: 48, Loss: 0.052333\n",
      "Epoch: 49, Loss: 0.044612\n",
      "Epoch: 50, Loss: 0.049658\n",
      "Epoch: 51, Loss: 0.183567\n",
      "Epoch: 52, Loss: 0.306800\n",
      "Epoch: 53, Loss: 0.067912\n",
      "Epoch: 54, Loss: 0.170536\n",
      "Epoch: 55, Loss: 0.045935\n",
      "Epoch: 56, Loss: 0.120636\n",
      "Epoch: 57, Loss: 0.064718\n",
      "Epoch: 58, Loss: 0.056445\n",
      "Epoch: 59, Loss: 0.040201\n",
      "Epoch: 60, Loss: 0.003565\n",
      "Epoch: 61, Loss: 0.042228\n",
      "Epoch: 62, Loss: 0.086206\n",
      "Epoch: 63, Loss: 0.042911\n",
      "Epoch: 64, Loss: 0.079548\n",
      "Epoch: 65, Loss: 0.018515\n",
      "Epoch: 66, Loss: 0.052856\n",
      "Epoch: 67, Loss: 0.009938\n",
      "Epoch: 68, Loss: 0.019014\n",
      "Epoch: 69, Loss: 0.017322\n",
      "Epoch: 70, Loss: 0.036108\n",
      "Epoch: 71, Loss: 0.035440\n",
      "Epoch: 72, Loss: 0.058770\n",
      "Epoch: 73, Loss: 0.032713\n",
      "Epoch: 74, Loss: 0.030325\n",
      "Epoch: 75, Loss: 0.035949\n",
      "Epoch: 76, Loss: 0.026623\n",
      "Epoch: 77, Loss: 0.026855\n",
      "Epoch: 78, Loss: 0.023253\n",
      "Epoch: 79, Loss: 0.012982\n",
      "Epoch: 80, Loss: 0.023739\n",
      "Epoch: 81, Loss: 0.047364\n",
      "Epoch: 82, Loss: 0.024903\n",
      "Epoch: 83, Loss: 0.015368\n",
      "Epoch: 84, Loss: 0.063199\n",
      "Epoch: 85, Loss: 0.032167\n",
      "Epoch: 86, Loss: 0.020840\n",
      "Epoch: 87, Loss: 0.012012\n",
      "Epoch: 88, Loss: 0.016365\n",
      "Epoch: 89, Loss: 0.016152\n",
      "Epoch: 90, Loss: 0.018456\n",
      "Epoch: 91, Loss: 0.041608\n",
      "Epoch: 92, Loss: 0.019386\n",
      "Epoch: 93, Loss: 0.022048\n",
      "Epoch: 94, Loss: 0.008092\n",
      "Epoch: 95, Loss: 0.014038\n",
      "Epoch: 96, Loss: 0.018485\n",
      "Epoch: 97, Loss: 0.032556\n",
      "Epoch: 98, Loss: 0.015819\n",
      "Epoch: 99, Loss: 0.010989\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss))) # Due to the shuffling, this now prints the loss for a random batch - \n",
    "                                                        # clearly something we want to improve in chapter 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999400\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.814\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(f\"Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.582256\n",
      "Epoch: 1, Loss: 0.561438\n",
      "Epoch: 2, Loss: 0.277775\n",
      "Epoch: 3, Loss: 0.524835\n",
      "Epoch: 4, Loss: 0.285204\n",
      "Epoch: 5, Loss: 0.666438\n",
      "Epoch: 6, Loss: 0.212069\n",
      "Epoch: 7, Loss: 0.394407\n",
      "Epoch: 8, Loss: 0.703533\n",
      "Epoch: 9, Loss: 0.508157\n",
      "Epoch: 10, Loss: 0.580842\n",
      "Epoch: 11, Loss: 0.299394\n",
      "Epoch: 12, Loss: 0.733760\n",
      "Epoch: 13, Loss: 0.327038\n",
      "Epoch: 14, Loss: 0.725558\n",
      "Epoch: 15, Loss: 0.443099\n",
      "Epoch: 16, Loss: 0.531968\n",
      "Epoch: 17, Loss: 0.607679\n",
      "Epoch: 18, Loss: 0.323157\n",
      "Epoch: 19, Loss: 0.240428\n",
      "Epoch: 20, Loss: 0.648723\n",
      "Epoch: 21, Loss: 0.164236\n",
      "Epoch: 22, Loss: 0.381451\n",
      "Epoch: 23, Loss: 0.494390\n",
      "Epoch: 24, Loss: 0.066966\n",
      "Epoch: 25, Loss: 0.312594\n",
      "Epoch: 26, Loss: 0.416677\n",
      "Epoch: 27, Loss: 0.274114\n",
      "Epoch: 28, Loss: 0.115102\n",
      "Epoch: 29, Loss: 0.093129\n",
      "Epoch: 30, Loss: 0.349315\n",
      "Epoch: 31, Loss: 0.103289\n",
      "Epoch: 32, Loss: 0.208464\n",
      "Epoch: 33, Loss: 0.071312\n",
      "Epoch: 34, Loss: 0.198908\n",
      "Epoch: 35, Loss: 0.176560\n",
      "Epoch: 36, Loss: 0.259543\n",
      "Epoch: 37, Loss: 0.224250\n",
      "Epoch: 38, Loss: 0.078512\n",
      "Epoch: 39, Loss: 0.049317\n",
      "Epoch: 40, Loss: 0.119823\n",
      "Epoch: 41, Loss: 0.111170\n",
      "Epoch: 42, Loss: 0.035962\n",
      "Epoch: 43, Loss: 0.049839\n",
      "Epoch: 44, Loss: 0.045945\n",
      "Epoch: 45, Loss: 0.036056\n",
      "Epoch: 46, Loss: 0.044208\n",
      "Epoch: 47, Loss: 0.011762\n",
      "Epoch: 48, Loss: 0.024060\n",
      "Epoch: 49, Loss: 0.010224\n",
      "Epoch: 50, Loss: 0.042148\n",
      "Epoch: 51, Loss: 0.001335\n",
      "Epoch: 52, Loss: 0.277778\n",
      "Epoch: 53, Loss: 0.008342\n",
      "Epoch: 54, Loss: 0.187649\n",
      "Epoch: 55, Loss: 0.005361\n",
      "Epoch: 56, Loss: 0.030870\n",
      "Epoch: 57, Loss: 0.028548\n",
      "Epoch: 58, Loss: 0.002954\n",
      "Epoch: 59, Loss: 0.009468\n",
      "Epoch: 60, Loss: 0.370491\n",
      "Epoch: 61, Loss: 0.022397\n",
      "Epoch: 62, Loss: 0.044850\n",
      "Epoch: 63, Loss: 0.301965\n",
      "Epoch: 64, Loss: 0.004755\n",
      "Epoch: 65, Loss: 0.002755\n",
      "Epoch: 66, Loss: 0.000120\n",
      "Epoch: 67, Loss: 0.040423\n",
      "Epoch: 68, Loss: 0.009236\n",
      "Epoch: 69, Loss: 0.030939\n",
      "Epoch: 70, Loss: 0.090626\n",
      "Epoch: 71, Loss: 0.018660\n",
      "Epoch: 72, Loss: 0.001500\n",
      "Epoch: 73, Loss: 0.001797\n",
      "Epoch: 74, Loss: 0.031239\n",
      "Epoch: 75, Loss: 0.000427\n",
      "Epoch: 76, Loss: 0.001809\n",
      "Epoch: 77, Loss: 2.285725\n",
      "Epoch: 78, Loss: 0.005348\n",
      "Epoch: 79, Loss: 0.002817\n",
      "Epoch: 80, Loss: 0.044322\n",
      "Epoch: 81, Loss: 0.002425\n",
      "Epoch: 82, Loss: 0.000862\n",
      "Epoch: 83, Loss: 0.011584\n",
      "Epoch: 84, Loss: 0.005200\n",
      "Epoch: 85, Loss: 0.005558\n",
      "Epoch: 86, Loss: 0.005930\n",
      "Epoch: 87, Loss: 0.001161\n",
      "Epoch: 88, Loss: 0.000932\n",
      "Epoch: 89, Loss: 0.002120\n",
      "Epoch: 90, Loss: 0.003958\n",
      "Epoch: 91, Loss: 0.010095\n",
      "Epoch: 92, Loss: 0.001457\n",
      "Epoch: 93, Loss: 0.002864\n",
      "Epoch: 94, Loss: 0.011898\n",
      "Epoch: 95, Loss: 0.002066\n",
      "Epoch: 96, Loss: 0.000207\n",
      "Epoch: 97, Loss: 0.000409\n",
      "Epoch: 98, Loss: 0.001762\n",
      "Epoch: 99, Loss: 0.005053\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.999900\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.803000\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1574402, [1572864, 512, 1024, 2])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "\n",
    "numel_list = [p.numel()\n",
    "              for p in first_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" So, at the end of this chapter, we have a dataset, a model, and a training loop, and\n",
    "our model learns. However, due to a mismatch between our problem and our network\n",
    "structure, we end up overfitting our training data, rather than learning the general-\n",
    "ized features of what we want the model to detect \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" We’ve created a model that allows for relating every pixel to every other pixel in\n",
    "the image, regardless of their spatial arrangement. We have a reasonable assumption\n",
    "that pixels that are closer together are in theory a lot more related, though. This\n",
    "means we are training a classifier that is not translation-invariant, so we’re forced to\n",
    "use a lot of capacity for learning translated replicas if we want to hope to do well on\n",
    "the validation set. There has to be a better way, right? \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" Of course, most such questions in a book like this are rhetorical. The solution to\n",
    "our current set of problems is to change our model to use convolutional layers. We\\u2019ll\n",
    "cover what that means in the next chapter. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
